---
layout: page
title: Research Areas
permalink: /research/
---



<table>
<thead>
<tr>
<div class="video-container">
<iframe width="560" height="315"  src="https://www.youtube.com/embed/U--BgrBPQfI?start=37&end=86&loop=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>
<th class="caption">Topic</th>
</tr>
</thead>

<tbody>

<tr>
<td class="topic">
<b>Interactive Learning Toward In-hand Manipulation of Deformable Objects</b>
<br>
<br>
    <a href="/assets/research/research_iitp.png" data-lightbox="Deformable object manipulation + Inverse constraint learning" >
      <img style="width: 50%" src="/assets/research/research_iitp.png">
    </a>
<br>
In-hand manipulation of deformable objects offers unprecedented opportunities to resolve
various real-world problems, such as binding and taping. This project aims to develop a visuotactile
in-hand manipulation that repositions/reorientations deformable objects in hand as we want. Toward
this line of research, we propose three research thrusts: 1) a physics-informed reinforcement learning
(RL) framework, 2) an interactive RL framework, and 3) Sim2Real transfer learning method.
<br>
<br>
<b>Keywords</b>: (Inverse) Reinforcement learning, Deformable obejct manipulation, Sim2Real transfer learning
<br>    
<b>Selected paper</b>: <a href="https://drive.google.com/open?id=1bswpgVJDXp_9vh55_Gz1cAbylhhjQqhS" target="_blank">[CoRL19]</a>, <a href="https://arxiv.org/abs/2306.12357" target="_blank">[RA-L23]</a> 
</td>
</tr>

<tr>
<td class="topic">
<b>Task-and-Motion Planning</b>
<br>
<br>
    <a href="/assets/research/research_TAMP.jpg" data-lightbox="Task-and-Motion Planning" >
      <img style="width: 50%" src="/assets/research/research_TAMP.jpg">
    </a>
<br>
We aim to introduce task-and-motion planning (TAMP) framework that is to solve complex and longer-time horizon of human tasks. To resolve completeness, optimality, and robustness issues, we are working on various task planning and motion planning approaches. We will show a generalizable TAMP framework under human operator’s cooperative or adversarial interventions.
<br>    
<br>
<b>Keywords</b>: Temporal logic, Neuro symbolic planning, Scene graph, Behavior tree, Collision avoidance
<br>        
<b>Selected paper</b>: <a href="https://drive.google.com/file/d/1cxN0KfKHJLfFXi0iLjhNREyjkqn46viG/view?usp=sharing" target="_blank">[ICRA21]</a>   <a href="https://drive.google.com/file/d/1cxN0KfKHJLfFXi0iLjhNREyjkqn46viG/view?usp=sharing" target="_blank">[RA-L22]</a>    
</td>
</tr>

<tr>
<td class="topic">
<b>Language-guided Quadrupedal Robot Navigation & Manipulation</b>
<br>
<br>
    <a href="/assets/research/research_quad.jpg" data-lightbox="Task-and-Motion Planning" >
      <img style="width: 50%" src="/assets/research/research_quad.jpg">
    </a>
<br>
Natural language is a convenient means to deliver a user’s high-level instruction. We introduce a language-guided manipulation framework that learns common-sense knowledge from natural language instructions and corresponding motion demonstrations. We apply the technologies on various quadrupedal robots like Boston Dynamics Spot!
<br>
<br>
<b>Keywords</b>: Quadruped robot, Semantic SLAM, Natural language grounding
<br>        
<b>Selected paper</b>: <a href="https://journals.sagepub.com/eprint/PSW4Z5AXF4AYTSXRN7AI/full" target="_blank">[IJRR20]</a><a href="http://fieldrobotics.net/Field_Robotics/Volume_2_files/Vol2_17.pdf" target="_blank">[FR22]</a>
</td>
</tr>


<tr>
<td class="topic">
<b>Machine Common Sense Learning for Robots</b>
<br>
<br>
    <a href="/assets/research/research_grounding.png" data-lightbox="Commonsense spatial grounding" >
      <img style="width: 30%" src="/assets/research/research_grounding.png">
    </a>
<br> Interpreting underspecified instructions re-quires environmental context and background knowledge about how to accomplish complex tasks. We investigate how to incorporate human-like commonsense knowledge for natural language understanding and task executions. 
<br>
<br>
<b>Keywords</b>: Large Language Models
<br>            
<b>Selected paper</b>: <a href="http://proceedings.mlr.press/v87/nyga18a/nyga18a.pdf" target="_blank">[CoRL18]</a>
</td>
</tr>

</tbody>
</table>


