---
layout: page
title: Research Areas
permalink: /research/
---


<td markdown="span">
    <a href="/assets/research/research_areas.png" data-lightbox="Research Areas" >
      <img style="width: 1000px" src="/assets/research/research_areas.png">
      </a>
</td>


<table>
<colgroup>
<col width="30%" />
<col width="70%" />
</colgroup>
<thead>
<tr>
<th class="caption">Topic</th>
<th class="caption">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
    <a href="/assets/research/2019_CoRL_CBN_IRL2.png" data-lightbox="corl19_cbnirl" >
      <img style="width: 350px" src="/assets/research/2019_CoRL_CBN_IRL_opt.png">
    </a>
</td>
<td class="description">
<b>Inverse Manipulation Skill Learning</b>
<!--![](//www.youtube.com/watch?v=HgaqH4PWcTI?width=100height=50)-->
<br>
Learning for manipulation is to obtain manipulation skills from a wide range of knowledge sources. We introduce methodologies for learning manipulation constraints and motion parameters from demonstrations.
<br>
<br>
<b>Selected paper</b>: Daehyung Park et al. "Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning", CoRL, 2019.
<a href="https://drive.google.com/open?id=1bswpgVJDXp_9vh55_Gz1cAbylhhjQqhS" target="_blank">[PDF]</a><a href="https://youtu.be/HgaqH4PWcTI" target="_blank">[Video]</a>
</td>
</tr>
<tr>
<td markdown="span">
    <a href="/assets/research/2021_RAL_LTL_BT.png" data-lightbox="ral20_ltl_bt" >
      <img style="width: 350px" src="/assets/research/2021_RAL_LTL_BT_opt.png">
      </a>
</td>
<td class="description">
<b>Dynamically Reconfigurable Task-and-Motion Planning</b>
<br> We present a robust task-and-motion planning(TAMP) framework under human operator’s cooperative oradversarial interventions.
</td>
</tr>

<tr>
<td markdown="span">
    <a href="/assets/research/2020_IJRR.png" data-lightbox="ijrr20" >
      <img style="width: 350px" src="/assets/research/2020_IJRR_opt.png">
      </a>
</td>
<td class="description">
<b>Natural Language Understanding for Manipulation</b>
<br>Natural language is a convenient means to deliver a user’s high-level instruction. We introduce a language-guided manipulation framework that learns common-sense knowledge from natural language instructions and corresponding motion demonstrations.
<br>
<br>
<b>Selected paper</b>: Daehyung Park#, Jacob Arkin# et al. "Multi-Modal Estimation and Communication of Latent Semantic Knowledge for Robust Execution of Robot Instructions", IJRR, 2020. (#- authors contributed equally) 
<a href="https://journals.sagepub.com/eprint/PSW4Z5AXF4AYTSXRN7AI/full" target="_blank">[PDF]</a><a href="https://www.youtube.com/watch?v=BfCeYsTvaOw&amp" target="_blank">[Video]</a>
</td>
</tr>

<tr>
<td markdown="span">
    <a href="/assets/research/2018_CORL.png" data-lightbox="CoRL2018" >
      <img style="width: 350px" src="/assets/research/2018_CORL_opt.png">
      </a>
</td>
<td  class="description">
<b>Machine Common Sense Learning for Robots</b>
<br>Interpreting underspecified instructions re-quires environmental context and background knowledge about how to accomplish complex tasks. We investigate how to incorporate human-like commonsense knowledge for natural language understanding and task executions. You can find related papers as follows,
<br>
<br>
<b>Selected paper</b>: Daniel Nyga et al. "Grounding Robot Plans from Natural Language Instructions with Incomplete World Knowledge", CoRL, 2018.
<a href="http://proceedings.mlr.press/v87/nyga18a/nyga18a.pdf" target="_blank">[PDF]</a><a href="https://youtu.be/uWv-l7XMoB8" target="_blank">[Video]</a>
</td>
</tr>

</tbody>
</table>


