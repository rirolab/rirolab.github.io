---
layout: page
title: Research Areas
permalink: /research/
---


<td markdown="span">
    <a href="/assets/research/research_areas.png" data-lightbox="Research Areas" >
      <img style="width: 1000px" src="/assets/research/research_areas.png">
      </a>
</td>


<table>
<colgroup>
<col width="30%" />
<col width="70%" />
</colgroup>
<thead>
<tr>
<th class="caption">Topic</th>
<th class="caption">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>
    <a href="/assets/research/2019_CoRL_CBN_IRL2.png" data-lightbox="corl19_cbnirl" >
      <img style="width: 350px" src="/assets/research/2019_CoRL_CBN_IRL_opt.png">
    </a>
</td>
<td class="description">
<b>Interactive Skill Learning Toward In-hand Manipulation of Deformable Objects</b>
<!--![](//www.youtube.com/watch?v=HgaqH4PWcTI?width=100height=50)-->
<br>
In-hand manipulation of deformable objects offers unprecedented opportunities to resolve
various real-world problems, such as binding and taping. This project aims to develop a visuotactile
in-hand manipulation that repositions/reorientations deformable objects in hand as we want. Toward
this line of research, we propose three research thrusts: 1) a physics-informed reinforcement learning
(RL) framework, 2) an interactive RL framework, and 3) Sim2Real transfer learning method.
<br>
<br>
<b>Keywords</b>: (Inverse) Reinforcement learning, Deformable obejct manipulation, Sim2Real transfer learning
<br>    
<b>Selected paper</b>: Daehyung Park et al. "Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning", CoRL, 2019.
<a href="https://drive.google.com/open?id=1bswpgVJDXp_9vh55_Gz1cAbylhhjQqhS" target="_blank">[PDF]</a><a href="https://youtu.be/HgaqH4PWcTI" target="_blank">[Video]</a>
</td>
</tr>
<tr>
<td markdown="span">
    <a href="/assets/research/2021_RAL_LTL_BT.png" data-lightbox="ral20_ltl_bt" >
      <img style="width: 350px" src="/assets/research/2021_RAL_LTL_BT_opt.png">
      </a>
</td>
<td class="description">
<b>Dynamically Reconfigurable Task-and-Motion Planning</b>
<br> We aim to introduce task-and-motion planning (TAMP) framework that is to solve complex and longer-time horizon of human tasks. To resolve completeness, optimality, and robustness issues, we are working on various task planning and motion planning approaches. We will show a generalizable TAMP framework under human operator’s cooperative or adversarial interventions.
<br>    
<br>
<b>Keywords</b>: Temporal logic, Neuro symbolic planning, Scene graph, Behavior tree, Collision avoidance
<br>        
<b>Selected paper</b>: Y. Kim et al. "GraphDistNet: A Graph-based Collision-distance Estimator for Gradient-based Trajectory," RA-L, 2022. <a href="https://drive.google.com/file/d/1cxN0KfKHJLfFXi0iLjhNREyjkqn46viG/view?usp=sharing" target="_blank">[PDF]</a><a href="https://youtu.be/lPpMVfBzZH0" target="_blank">[Video]</a>    
</td>
</tr>

<tr>
<td markdown="span">
    <a href="/assets/research/2020_IJRR.png" data-lightbox="ijrr20" >
      <img style="width: 350px" src="/assets/research/2020_IJRR_opt.png">
      </a>
</td>
<td class="description">
<b>Language-guided Quadrupedal Robot Navigation & Manipulation</b>
<br>Natural language is a convenient means to deliver a user’s high-level instruction. We introduce a language-guided manipulation framework that learns common-sense knowledge from natural language instructions and corresponding motion demonstrations. We apply the technologies on various quadrupedal robots like Boston Dynamics Spot!
<br>
<br>
<b>Keywords</b>: Quadruped robot, Semantic SLAM, Natural language grounding
<br>        
<b>Selected paper</b>: Daehyung Park#, Jacob Arkin# et al. "Multi-Modal Estimation and Communication of Latent Semantic Knowledge for Robust Execution of Robot Instructions", IJRR, 2020. (#- authors contributed equally) 
<a href="https://journals.sagepub.com/eprint/PSW4Z5AXF4AYTSXRN7AI/full" target="_blank">[PDF]</a><a href="https://www.youtube.com/watch?v=BfCeYsTvaOw&amp" target="_blank">[Video]</a>
</td>
</tr>

<tr>
<td markdown="span">
    <a href="/assets/research/2018_CORL.png" data-lightbox="CoRL2018" >
      <img style="width: 350px" src="/assets/research/2018_CORL_opt.png">
      </a>
</td>
<td  class="description">
<b>Machine Common Sense Learning for Robots</b>
<br>Interpreting underspecified instructions re-quires environmental context and background knowledge about how to accomplish complex tasks. We investigate how to incorporate human-like commonsense knowledge for natural language understanding and task executions. You can find related papers as follows,
<br>
<br>
<b>Keywords</b>: 
<br>            
<b>Selected paper</b>: Daniel Nyga et al. "Grounding Robot Plans from Natural Language Instructions with Incomplete World Knowledge", CoRL, 2018.
<a href="http://proceedings.mlr.press/v87/nyga18a/nyga18a.pdf" target="_blank">[PDF]</a><a href="https://youtu.be/uWv-l7XMoB8" target="_blank">[Video]</a>
</td>
</tr>

</tbody>
</table>


